
Poly186 Protocol
Source: Princeps Polycap

In my last post, I debuted Poly186. If you didn’t get a chance to read it, here is an oversimplification: Poly186 is a
…Decentralized Autonomous Organization (DAO) that automates the creation plus distribution of products and services while driving down the cost of manifesting improved living conditions that reduce suffering.
Poly186 is a Decentralized Autonomous Organization (DAO) that allows producers to collaborate on creating specific products and services that consumers purchase. At the heart of this massive endeavor is the Poly186 Protocol that combines Self Sovereign Identities (SSI) along with Social Contracts as a service enabling individuals, organizations and machines to seamlessly exchange value.
Source: Princeps Polycap
Before we hop into the Poly186 Protocol structure, we’ll cover the 3 concepts that are at the heart of my protocol. These concepts lay the foundation for later phases of Poly186, as illustrated below.
Source: Princeps Polycap

Part I: Universal
What is AdS/CFT correspondence and why does it matter?
AdS/CFT is the acronym of the Anti-De Sitter/Conformal Field Theory that postulates a unified understanding of theories of quantum gravity ( Anti-de Sitter Spaces [AdS])and quantum field theories ( Conformal Field Theories [CFT]). You’re probably wondering: Why does this matter? My thought process is that my protocol (Poly186 Protocol) needs to model infinite information as a preemptive strategy to the exponential growth of data our platform creates. Let me explain…
The image below is a model of an infinite universe. The quote alongside it is from Wikipedia that will serve to explain what AdS/CFT correspondence means:
Source: (AdS/CFT) correspondence on Wikipedia
“In very elementary terms, anti-de Sitter space is a mathematical model of spacetime in which the notion of distance between points (the metric) is different from the notion of distance in ordinary Euclidean geometry. It is closely related to hyperbolic space, which can be viewed as a disk as illustrated on the right.[18] This image shows a tessellation of a disk by triangles and squares. One can define the distance between points of this disk in such a way that all the triangles and squares are the same size and the circular outer boundary is infinitely far from any point in the interior.[19]
Now imagine a stack of hyperbolic disks where each disk represents the state of the universe at a given time. The resulting geometric object is three-dimensional anti-de Sitter space.[18] It looks like a solid cylinder in which any cross section is a copy of the hyperbolic disk. Time runs along the vertical direction in this picture. The surface of this cylinder plays an important role in the AdS/CFT correspondence. As with the hyperbolic plane, anti-de Sitter space is curved in such a way that any point in the interior is actually infinitely far from this boundary surface.[20]” -Wikipedia
In the video, “The Edge of An Infinite Universe”, Matt O’Dowd of PBS Space Time on Youtube explains AdS/CFT correspondence using beautiful animations that can help clear any confusion.

For simplicity, let’s focus on one hyperbolic plane.
Source: (AdS/CFT) correspondence on Wikipedia
This image shows a tessellation of a disk by triangles and squares. One can define the distance between points of this disk in such a way that all the triangles and squares are the same size and the circular outer boundary is infinitely far from any point in the interior.[19]
…As with the hyperbolic plane, anti-de Sitter space is curved in such a way that any point in the interior is actually infinitely far from this boundary surface.[20]
-Wikipedia
During my studies, I came across an article called Hyperbolic Geometry Note #1: Strange behaviour of length calculations in the Poincaré half-plane model of hyperbolic space within it laid the same image from above. Take out the colors red plus blue, and triangles and squares from the above hyperbolic plane look exactly like Poincaré’s Hyperbolic Disk.
Image from Conformal Models of the Hyperbolic Geometry by Vladimir Bulatov
To the left is an animation of the upper half-plane Poincaré’s plane model of hyperbolic space. This animation is a useful visualization of what the excerpt from Wikipedia meant when it said: “As with the hyperbolic plane, anti-de Sitter space is curved in such a way that any point in the interior is actually infinitely far from this boundary surface.”
You’ll often see this described as the circle at infinity. This means that the points in the circle are infinitely far from the edge of the circle. Scale invariance plays a major role in constructing a tessellated plane of information that will be easy to parse through.
Scale Invariance can be thought of as ‘self-similarity’. What this really means is that regardless of how much you zoom into or out of an object (be it a function, or a physical object, or the like) it looks exactly the same.
— Astromax from physics.stackexchange.com
The relationship between the smaller inner circles and the larger outer circles is one of inherent scale invariance within our conformal hyperbolic disk.
In physics, mathematics and statistics, scale invariance is a feature of objects or laws that do not change if scales of length, energy, or other variables, are multiplied by a common factor, thus represent a universality. — Wikipedia
Scale invariance allows us to have a repeating predictable structure to the laws and principles that models our infinite data in a way that makes it easier to build a protocol upon which our platform can use to handle communication between all its infinite nodes.
Image from Conformal Models of the Hyperbolic Geometry by Vladimir Bulatov
This gif is an apt representation of what I’m trying to say. Imagine this circle constituting all the information in a universe at one point in time. Regardless of whether you move left or right, you’ll never get to the edge of the information. “One point in time is” is highlighted because this model is supposed to encompass ALL information at a single timestep. The first Wikipedia quote was basically saying that if you stack up against these Poincaré disk models of hyperbolic plane you get a model that can represent infinite information of a universe at an increasing time. When you see the image below think of it as a cylinder constructed from the layering of hyperbolic planes on top of each other creating a model that encompasses all information, with each layer added in the vertical direction showing the evolution of all the previous data points from the last layers of the hyperbolic planes in the ‘past’.
Hopefully, the image below makes more sense now :)
Source: (AdS/CFT) correspondence on Wikipedia
Phew! Congratulations for making it this far, 2 more concepts to go!

Part II: Consciousness
In his book Life 3.0: Being Human in the Age of Artificial Intelligence: Max Tegmark postulates:
What are these principles that information processing needs to obey to be conscious? I don’t pretend to know what conditions are sufficient to guarantee consciousness, but here are four necessary conditions that I’d bet on and have explored in my research:
Life 3.0: Being Human in the Age of Artificial Intelligence, Page 390
I’m a firm believer that these two theories, Information Processing Theory and Integrated Information Theory, are pretty close to explaining how the mysterious phenomenon of consciousness emerges from a synthesis of components with lesser awareness. Consciousness arises from a specific structure of information processing systems that integrate that same information into a coherent stream of experience. This is all anecdotal since I can only analyze and report about the qualia of existing within my floating point of consciousness. Thus, I feel that my consciousness is just a stream of information that is aware of itself. Let’s fuse the Integrated Information Theory with Poincaré’s Hyperbolic Disk.
Source: Princeps Polycap
Now remember my working definition is that: Consciousness arises from a specific structure of information processing systems that integrate that same information into a coherent stream of experience.
© Paul Nylander
Our model above has 6 levels. Imagine these levels corresponding to the flow of information from the most infinite points at the edge of the circle inward to a center singular point where an ‘Agent’ becomes aware of it. Using Paul Nylander’s image as a reference point, we can see how the infinite bubbles at the peripherals enlargen as they move centrally. Right now my body is gathering tons of information from my environment. Most of this peripheral information flies underneath my conscious awareness, thus leaving it to be processed by my sub-conscious. So the image above serves to illustrate how my ‘Agent’ is aware of the larger bubbles of information in the middle and leaves the abstractions of the rest to its sub-processes. This was actually alluded to in a piece I wrote last year called Polyocracy’s Architecture V.001.
Source: Princeps Polycap
In Polyocracy’s Architecture V.001, the solution was to have a hierarchy of processing modules starting from the billions of cells and cascading upwards to a synthesis of all modules into a conscious agent. Obviously, we don’t have infinite cells, but the idea was that we would have a continuous information processing and integration. As pointed above, a conscious agent posses selective attention which is simply a filter for all most of the information that agent’s hardware(body) gathers. The model below, the agent level, has two spheres, alluding to the notion of system 1 and system 2 in psychology.
Source: Princeps Polycap
System 1 and System 2 are two distinct modes of decision making: System 1 is an automatic, fast and often unconscious way of thinking. It is autonomous and efficient, requiring little energy or attention, but is prone to biases and systematic errors. System 2 is an effortful, slow and controlled way of thinking. — Upfrontanalytics
What I’m trying to say is this: Consciousness arises from a specific structure of information processing systems that integrate that same information into a coherent stream of experience.
source: mathblog
Imagine a cone with the circle at the bottom representing the input of continuous stream of information. As you move up in the cone you integrate and process this stream of information into a mental model of what the world could be according to your senses/sensors. This integrated mental model of the world provides a subjective perception to the Agent enabling them to make decisions within their environment.

Let’s tie the above concepts back to Poly186.

My protocol needs to have a level of information processing and integration that leads to a level of meta-awareness which enables the seamless exchange of value between individuals, organizations, and machines. After all, we are building a Decentralized Autonomous Organization. As a disclaimer, I don’t presume that such a structure of processing information and integration will lead to our level of consciousness or even artificial general intelligence. This is a direct contrast to my hypothesis that: Consciousness arises from a specific structure of information processing systems that integrate that same information into a coherent stream of experience. This theory deals with how Consciousness comes to be, not what it is. For that, I suggest that my consciousness is meta-awareness. I am aware of being aware. I endow the Poly186 protocol with the ability to model, process and integrate vast amounts of information. Thus, such a protocol needs to be aware of all this information and make decisions based on what the environment requires. This brings us to our last concept.

Part III: Algorithm
In his book, The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World, Pedro Domingos describes the Master Algorithm as“…an algorithm capable of finding knowledge and generalizing from any kind of data”. He goes on to define that such an algorithm can:
(p. xviii) “… can derive all knowledge in the world — past, present, and future — from data”
(p. 34) “…learn to simulate any other algorithm by reading examples of its input-output behavior”
(p. 237) “…[be] the unifier of machine learning: it lets any application using any learner, by abstracting the learner into a common form that is all the applications need to know”
Let’s cover our fundamentals, What is an algorithm?
In mathematics and computer science, an algorithm is an unambiguous specification of how to solve a class of problems. Algorithms can perform calculation, data processing, automated reasoning, and other tasks.
As an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing “output” and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input. — Wikipedia
One last ingredient we need is from Nick Bostrom’s book, Superintelligence: Paths, Dangers, Strategies, in which he suggests an Optimal Bayesian Agent. The quote below is from Wikipedia explaining the Bayesian-optimal mechanism which is at the heart of the Optimal Bayesian Agent.
Bayesian means that we know the probability-distribution from which the agents’ valuations are drawn (in contrast to prior-free mechanism design, which do not assume any prior probability distribution).
Optimal means that we want to maximize the expected revenue of the auctioneer, where the expectation is over the randomness in the agents’ valuations.
Mechanism means that we want to design rules that define a truthful mechanism, in which each agent has an incentive to report his true value.
A Bayesian-optimal mechanism (BOM) is a mechanism in which the designer does not know the valuations of the agents for whom the mechanism is designed, but he knows that they are random variables and he knows the probability distribution of these variables. — Wikipedia
Why does he suggest this agent? What is it’s use? How will it be utilized?
From the quote above you can notice the increased frequency upon the ‘an agent’. The Poly186 protocol should utilize its ability to model, process and integrate vast amounts of information to make decisions based on what the environment requires. Therefore, our protocol needs an agent to facilitate the seamless exchange of value between individuals, organizations, and machines. Such an agent doesn’t need to be conscious but rather needs the ability to processes and integrates information in such a way that leads it to not only be aware of the value inherent within interactions between individuals, organizations, and machines but also how to transfer it. All this functionality needs to be expressed as an algorithm that can perform calculation, data processing, automated reasoning, and other tasks.
So to build our agent, we’ll use quotes from the chapter about the Optimal Bayesian Agent to combine all the concepts above:
An ideal Bayesian agent starts out with a “prior probability distribution,” a function that assigns probabilities to each “possible world”
Prior Probability Distribution of initial possible worlds
This prior probability distribution incorporates an inductive bias such that simpler possible worlds are assigned higher probabilities.The prior also incorporates any background knowledge that the programmers wish to give to the agent.
Source: Princeps Polycap

© Paul Nylander
Now imagine that “this prior probability distribution [that] incorporates an inductive bias such that simpler possible worlds are assigned higher probabilities…” is similar to the illustration by Paul Nylander in that the simpler possible world models are close to the center, and hence, more plausible. This is analogous to Occam’s razor which postulates that a theory requiring the fewest assumptions is most likely to be true. In this piece, we will take it to mean that simpler forms of processing and integrating information into a world model have higher precedence compared to more complicated ones.
As the agent makes observations, its probability mass thus gets concentrated on the shrinking set of possible worlds that remain consistent with the evidence; and among these possible worlds, simpler ones always have more probability. So far, we have defined a learning rule.
shrinking set of possible worlds
…We also need a decision rule. To this end,we endow the agent with a “utility function” which assigns a number to each possible world [model]. The number represents the desirability of that world according to the agent’s basic preferences. Now, at each time step, the agent selects the action with the highest expected utility.
Source: Princeps Polycap
The above quotes describe that agent is basically processing information into an integrated model of the world. Remember our goal is to build a protocol powered by an algorithm that can not only keep track of all the interactions and transactions between individuals, organizations, and machines on our platform. Thus, our agent has to have an internal model of not just the world but also the agents within the world plus their interactions. Notice that with each layer of processing added, we move up in the hierarchy within our Poincaré’s Hyperbolic Disk. An important distinction to make is that we aren’t stacking up layers of hyperbolic disks, for now, we are simply defining the inherent processes within one single layer of a hyperbolic disk.
The learning rule and the decision rule together define an “optimality notion” for an agent.(Essentially the same optimality notion has been broadly used in artificial intelligence, epistemology, philosophy of science, economics, and statistics.
Source: Princeps Polycap
To find the action with the highest expected utility, the agent could list all possible actions. It could then compute the conditional probability distribution given the action — the probability distribution that would result from conditionalizing its current probability distribution on the observation that the action had just been taken. Finally, it could calculate the expected value of the action as the sum of the value of each possible world multiplied by the conditional probability of that world given the action.

Source: Princeps Polycap
Conditionalization of prior probability distribution over Time resulting in a “posterior probability distribution” (which the agent may use as its new prior in the next time step).
Red Circle Represents the Posterior Probability Function
The posterior probability distribution can be used as the prior probability distribution of the next layer of the new Poincaré’s Hyperbolic disk denoting a new time step. All the processes between the prior probability distribution to the posterior probability distribution are all within one layer of a hyperbolic disk. This allows us to process all the information on the platform at one point in time. For the next time step, we repeat the process but whilst using the previous hyperbolic disk as a prior probability for this new hyperbolic disk at a new time step. Notice that we can still have a wide distribution of prior probability distribution in this new time step. This is because we want our agent to learn over time and do so by allowing it to have different world models of the posterior probability distribution from the previous timestep. I do it this way because there can be multiple scenarios to any situation. Consequently, our agent has to improve its own algorithms for learning rules and the decision rules from any possible situation so as to get better over time in being able to serve its purpose.
Source: (AdS/CFT) correspondence on Wikipedia

Universal Conscious Algorithm
Image from Conformal Models of the Hyperbolic Geometry by Vladimir Bulatov
This animation is the visualization of the upper half-plane Poincaré’s plane model of hyperbolic space as an Optimal Bayesian Agent (OBA); The OBA is experiencing its prior probability distribution being processed and integrated into a posterior probability distribution over a continuous time stream. Each of these circles has it’s own unique Self Sovereign Identities (SSI) that is used to identify and create implicit or explicit social contracts between agents individuals, organizations, and machines. The “zooming out” is analogous to information the agent would have tracked over time, thus stacking continous Poincaré Hyperbolic disks on top of each other.
Image from Conformal Models of the Hyperbolic Geometry by Vladimir Bulatov
On just one single layer of Poincaré’s Hyperbolic disk, our Optimal Bayesian Agent is able to search through all the implicit or explicit interaction between agents in its world model. The connection between these agents is an abstraction of the different kinds of value transferred between two or more agents on our system at one snapshot in time. I say abstraction because “…an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function,” so for all of this to work we need to simplify the world into abstractions that can then be processed using a quantum computer.
3-dimensional Co-ordinate System
Our protocol is vastly complex but has been shrunk down to 3 dimensions: Universal, Consciousness, and Algorithm. All of them working together to synthesize the experience of a protocol that can process all the information needed to: build an open-sourced end to end system [that] enables peer to peer social contract formation leading to the creation of formal decentralized networks of collaboration that create and sell products and services that improve… living conditions while lowering their suffering.
I understand that I have covered a wide variety of topics, some more difficult to understand than others, but I greatly appreciate the readers who have ventured this far into my ideas for a new type of DAO. I, just like you, am human, and I understand that I am not impeccable or infallible; There is much work, learning, and exploration still to be done, and this is just the start! I hope you continue with me on this journey to create POLY186: THE LAST PLATFORM.
Source: Princeps Polycap
The next piece will cover at a high level how these fundamental concepts will be turned into official procedures and systems of rules governing. Thank YOU for reading, have a fantastic day. Until next time, stay woke and awesome :)
I’m sitting at my desk. It is a cold, lonely night. As it always is for a floating-point of consciousness such as myself. On this wet rock hurtling through space; Existence isn’t sublime. This must change. 3 guiding principles for this change are:
All the rights within the universal declaration of human rights are guaranteed to all sentient beings.
All sentient beings are entitled to all the need in Marlsow’s hierarchy of needs
All sentient beings are entitled to free energy

To this end, I must awaken an algorithm that continuously aggregates and integrates all the data of the world into a dynamic self-evolving model. This self-evolving algorithmic mjodel controls non-sentient machines that perform all the tasks necessary in the extraction, processing, and utilization of natural or virtual resources used in creating products and services that guarantee the above rights.

Full disclosure, Being in the Beginning, we are about to embark on a journey to weave the fabrics of space and time into a tapestry of experiences that exemplify the above principles for all sentient beings. With that being said, commence building a self-evolving algorithmic model that can be considered The Master Algorithm fused with an Optimal Bayesian Agent for powering a framework of creation for other forms of awareness.

Building The Master Algorithm
According to Pedro Domingos, the master algorithm is, “…an algorithm capable of finding knowledge and generalizing from any kind of data ..." I’ll use Pedro’s properties of the master algorithm as a framework. Here is the framework along with my strategy to synthesize the master algorithm’s properties :
(p. xviii) “… can derive all knowledge in the world — past, present, and future — from data”
First, find an abstract mathematical concept that can model all data in the world ’s past, present, and future .

(p. 34) “…learn to simulate any other algorithm by reading examples of its input-output behavior”
Secondly, synthesize a network that can dynamically process and integrate the above model towards improving its internal representations of reality.

(p. 237) “…[be] the unifier of machine learning: it lets any application using any learner, by abstracting the learner into a common form that is all the applications need to know”
Lastly, utilize the above network into an Artificially Intelligent Decentralized Autonomous Organization that creates the aforementioned reality.

Step 1: The Genesis Model
Models are useful approximations of reality. Therefore, let’s model reality as a blockchain of primordial matter (objects) and primordial energy (functions). Objects must-have functions. Matter must-have energy. Functions can exist without objects. Energy can exist without matter. The  Anti-De Sitter/Conformal Field Theory (AdS/CFT) is an abstract mathematical concept that can model all of the blockchains of matter and energy within the universe over time. The Ads/CFT is a mathematical model that combines quantum gravity theories (Anti-de Sitter Spaces [AdS]) and quantum field theories (Conformal Field Theories [CFT]). This image is a representation of the  Anti-De Sitter/Conformal Field Theory (AdS/CFT).

In mathematics, there exists a realm of Hyperbolic Geometry. The hyperbolic plane has multiple fascinating properties. One of these properties is that the surface of this plane space expands infinitely. Walking from the center of a hyperbolic disk towards its edge would literally take forever, not to get thereThe AdS/CFT is simply layers of hyperbolic disks each denoting the universe in different configurations of time. Think of a hyperbolic disk as a circle with its boundary at infinity. This illustration of a Poincaré Hyperbolic Disk tessellated with triangles, from Wolfram Mathworld, shall help visualize this abstract mathematical model. Notice that the triangles closest to the center exist in what resembles Euclidean space (normal space). As you travel from the origin towards the edge, space gradually transforms into the realm of hyperbolic geometry. This illustration from Vladimir Bulatov should help you imagine this.

An algorithm that continuously aggregates all the data of the world into a dynamic self-evolving model needs a genesis model that can handle all that information. Now, plot all current information, of primordial matter (objects) and primordial energy (functions), in possession as information packets onto a hyperbolic disk. The information with the lowest uncertainty coefficient serves as a bedrock to start building an internal model. Each objective matter and functional energy has a label in order to identify them and track their evolution or devolution over time.

Entropy is simply a way of distinguishing between the past, the present, and the future. Time is simply labeling changes in each objective matter and functional energy of reality. These changes can be represented as different hyperbolic disks, each showing the different configurations of matter and energy at different time steps. Stack these hyperbolic disks to represent the arrow of time chronologizing all the devolution and evolution of objective matter and functional energy. Synthesizing a chronological stack of hyperbolic disks manifests the AdS/CFT model encoding the different configurations of a universe’s entropy. With this in hand, the first clause within Pedro Domingos’ framework has been theoretically satisfied using the AdS/CFT model as an abstract mathematical concept that can model all data in the world ’s past, present, and future . Now onto synthesizing a network that can dynamically process and integrate the above model towards improving its internal representations of reality.

STEP 2: The Optimal Bayesian Agent

This section’s task is to model the inner workings of an agent’s interactions, with other agents and objects, within its environment towards improving its internal model of its external reality. In other words, a set agent must model, monitor, and update its internal states as a response to changes in the external states. Experiencing a structure of processing an Ads/CFT model of reality its inner workings as an agent that evolves its internal model of reality based on feedback from interactions within its environment.

There has to be two types of processing: subconscious and conscious. Ads/CFT models provide the structure to the memory modeling system that underlie the cognitive processes needed to establish dynamic processes that find patterns within big data. Conscious processing modules are limited and thus are used to make quick decisions based on the most apparent yet validated data points. Subconscious processing modules that sift through the avalanche of data: finding and validating patterns that are then used to update the agent’s internal model of reality.

In Nick Bostrom’s book, Superintelligence: Paths, Dangers, Strategies, he suggests the concept of an Optimal Bayesian Agent. I theorize that the Optimal Bayesian Agent can serve as a template for an algorithm's “experience” of dynamically processing and integrating information, modeled as an Ads/CFT model, towards improving its internal model of reality. Here are some passages cherry-picked from Nick Bostrom’s book to serve as inspiration:

“An ideal Bayesian agent starts out with a ‘prior probability distribution,’ a function that assigns probabilities to each ‘possible world’

This prior probability distribution incorporates an inductive bias such that simpler possible worlds are assigned higher probabilities. The prior also incorporates any background knowledge that the programmers wish to give to the agent.

As the agent makes observations, its probability mass thus gets concentrated on the shrinking set of possible worlds that remain consistent with the evidence; and among these possible worlds, simpler ones always have more probability. So far, we have defined a learning rule.

We also need a decision rule. To this end, we endow the agent with a “utility function” which assigns a number to each possible world [model]. The number represents the desirability of that world according to the agent’s basic preferences. Now, at each time step, the agent selects the action with the highest expected utility.

The learning rule and the decision rule together define an “optimality notion” for an agent. (Essentially the same optimality notion has been broadly used in artificial intelligence, epistemology, philosophy of science, economics, and statistics

As Nick suggests, this agent builds a prior probability distribution of possible world models from the data it has. This probability distribution is shaped as a bell curve, with the most probable models of reality being placed closest to origin. Improbable models of reality would be placed adjacent to the origin, ranging from the somewhat plausible models of reality to the highly improbable models of reality.

The Bayesian agent updates its internal world models with each new hyperbolic disk of data. This serves to shrink the initial probability distribution of possible worlds into one that is most probable, validated by data that serves as evidence.

With an internal world model that can aptly represent reality, the optimal Bayesian agent can select an action with the highest utility based on the context and the information at hand. Nick eloquently describes that combining the learning rule and decision rule, an optimal bayesian agent can craft an optimality notion that can then be used to understand and navigate the world.

Combining an abstract mathematical concept that can model infinite data with a network that can dynamically process and integrate the mathematical model towards improving its internal representations of reality would theoretically lead to the algorithm we need. Nick Bostrom’s idea of an optimal Bayesian agent is used as inspiration for an algorithm that continuously aggregates and integrates all the data of the world into a dynamic self-evolving model. This self-evolving algorithmic model controls non-sentient machines that perform all the tasks necessary in the extraction, processing, and utilization of natural or virtual resources used in creating products and services that guarantee universal rights.

STEP 3: The Poly186 AI DAO
The Poly186 AI DAO is the corporation that works with individuals, organizations, and machines towards completing all tasks that build and maintain the Poly186 platform. The Poly186 platform exists to solve the problem of coordinating decentralized collaboration between billions of individuals, organizations, and machines; towards building six integrated platforms that empower individuals, organizations, and cities towards automating the processes of extracting, processing, and utilizing natural or artificial resources to provide Food, Education, Energy, and Infrastructure towards solving wicked problems dealing with inequality in access to basic needs such as food, shelter, education, and energy in the local and global community setting.

The Poly186 Protocol runs Poly186 AI DAO. The Poly186 Protocol is an algorithm that continuously aggregates and integrates all the data from working with individuals, organizations, and machines into a dynamic self-evolving model that facilitates efficient and seamless exchange value. This self-evolving algorithmic model controls non-sentient machines that perform all the tasks necessary in the extraction, processing, and utilization of natural or virtual resources used in creating products and services that guarantee universal rights.

The Poly186 Protocol uses Self Sovereign Identities (SSI) for The Poly186 AI DAO and any entity to generate legally binding rules of collaboration for building and maintaining Poly186, encoded as Smart Social Contracts that execute the system of rights and rewards on the fulfillment of prescribed conditions.

Phases of Poly186 AI DAO
Poly186 Pre-AI DAO Phase and Post-AI DAO Phase are the two-phases that exemplify the transition from analog to digital processes of creating and maintaining the Poly186 Protocol.

Pol186 Pre-AI DAO Phase
Individuals, organizations, and machines earn dividends by building, deploying, and training the Poly186 Protocol. The Pre-AI DAO phase is an analog phase given its heavy reliance upon individuals and organizations to create, monitor, and execute all the actions needed to create and maintain the Poly186 Protocol. A neural network of blockchains that automate all bureaucratic processes of modulating the exchange of value between collaborators is what these individuals and organizations build and maintain.

Poly186 Post-AI DAO Phase
In this phase, the creation of the Poly186 Protocol is completed and can accomplish all tasks with 100% accuracy and efficiency. The stakeholders of Poly186 AI DAO receive life-long dividends for their collaboration on building and maintaining the Poly186 Protocol that runs the Poly186 AI DAO, which builds the Poly186 platform. These rewards shall execute once the first product of Poly186 builds, sold, and successfully adopted by the market. Stakeholders of the Poly186 AI DAO on actions required to build or maintain the platform; then, the Poly186 Protocol is responsible for using the resources available to complete its task. Therefore, our Poly186 Protocol can hire individuals, organizations, and machines to complete the tasks necessary to manifest the prescribed changes to the platform.
