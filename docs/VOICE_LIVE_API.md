# Voice Live API Architecture

## Overview

Odelle uses the **Azure OpenAI Realtime API** (`/realtime` endpoint) for real-time voice interactions via WebSocket. This provides low-latency, speech-in/speech-out conversational capabilities with GPT-4o.

## Three Core Modes

### 1. Live Conversation Mode
Full bidirectional voice conversation with AI responding in audio.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     audio (pcm16)     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User      â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚  Azure OpenAI   â”‚
â”‚   (mic)     â”‚                       â”‚  Realtime API   â”‚
â”‚             â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚                 â”‚
â”‚   (speaker) â”‚     audio + text      â”‚  (gpt-realtime) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Session Config:**
```json
{
  "type": "session.update",
  "session": {
    "modalities": ["text", "audio"],
    "voice": "alloy",
    "turn_detection": { "type": "server_vad" },
    "input_audio_format": "pcm16",
    "output_audio_format": "pcm16",
    "input_audio_transcription": { "model": "whisper-1" }
  }
}
```

**Use Case:** Kitchen Confessional conversations, AI coaching

---

### 2. Transcription-Only Mode
Voice input â†’ text transcription, no AI audio response.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     audio (pcm16)     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User      â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚  Azure OpenAI   â”‚
â”‚   (mic)     â”‚                       â”‚  Realtime API   â”‚
â”‚             â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚                 â”‚
â”‚             â”‚     transcription     â”‚  (whisper-1)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         only          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Session Config:**
```json
{
  "type": "session.update",
  "session": {
    "modalities": ["text"],
    "turn_detection": { "type": "server_vad" },
    "input_audio_format": "pcm16",
    "input_audio_transcription": { "model": "whisper-1" }
  }
}
```

**Use Case:** Voice journaling, note-taking, quick captures

---

### 3. Tool Calling Mode (Agent)
Real-time voice with function/tool calling capabilities.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     audio + tools     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User      â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚  Azure OpenAI   â”‚
â”‚   (voice)   â”‚                       â”‚  Realtime API   â”‚
â”‚             â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚                 â”‚
â”‚             â”‚  audio + tool_calls   â”‚  + tool defs    â”‚
â”‚             â”‚                       â”‚                 â”‚
â”‚   App       â”‚ â”€â”€â”€ tool_response â”€â”€â†’ â”‚                 â”‚
â”‚             â”‚ â†â”€â”€ continue resp â”€â”€  â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Session Config:**
```json
{
  "type": "session.update",
  "session": {
    "modalities": ["text", "audio"],
    "voice": "alloy",
    "turn_detection": { "type": "server_vad" },
    "input_audio_format": "pcm16",
    "output_audio_format": "pcm16",
    "input_audio_transcription": { "model": "whisper-1" },
    "tools": [
      {
        "type": "function",
        "name": "save_journal_entry",
        "description": "Saves a journal entry to the database",
        "parameters": {
          "type": "object",
          "properties": {
            "content": { "type": "string", "description": "Journal entry text" },
            "mood": { "type": "string", "enum": ["happy", "sad", "neutral", "anxious", "excited"] }
          },
          "required": ["content"]
        }
      }
    ]
  }
}
```

**Use Case:** Voice commands, AI agent actions, workflow automation

---

## WebSocket Protocol

### Connection URL
```
wss://<resource>.cognitiveservices.azure.com/openai/realtime
  ?api-version=2024-10-01-preview
  &deployment=<deployment-name>
  &api-key=<api-key>
```

### Message Flow

```
Client                                   Server
  â”‚                                         â”‚
  â”‚â”€â”€â”€â”€ [connect WebSocket] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚
  â”‚                                         â”‚
  â”‚â†â”€â”€â”€â”€ session.created â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚                                         â”‚
  â”‚â”€â”€â”€â”€â”€ session.update â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚ (configure session)
  â”‚â†â”€â”€â”€â”€ session.updated â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚                                         â”‚
  â”‚â”€â”€â”€â”€â”€ input_audio_buffer.append â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚ (stream audio)
  â”‚â”€â”€â”€â”€â”€ input_audio_buffer.append â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚
  â”‚â”€â”€â”€â”€â”€ input_audio_buffer.append â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚
  â”‚                                         â”‚
  â”‚â†â”€â”€â”€â”€ input_audio_buffer.speech_started â”€â”‚ (VAD detected speech)
  â”‚                                         â”‚
  â”‚â†â”€â”€â”€â”€ input_audio_buffer.speech_stopped â”€â”‚ (VAD detected silence)
  â”‚                                         â”‚
  â”‚â†â”€â”€â”€â”€ conversation.item.created â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚â†â”€â”€â”€â”€ conversation.item.audio_           â”‚
  â”‚      transcription.completed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ (user transcription)
  â”‚                                         â”‚
  â”‚â†â”€â”€â”€â”€ response.created â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚â†â”€â”€â”€â”€ response.output_item.added â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚â†â”€â”€â”€â”€ response.audio.delta â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ (streaming audio)
  â”‚â†â”€â”€â”€â”€ response.audio_transcript.delta â”€â”€â”‚ (streaming text)
  â”‚â†â”€â”€â”€â”€ response.done â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚                                         â”‚
```

---

## Key Events Reference

### Server â†’ Client Events

| Event | Description |
|-------|-------------|
| `session.created` | Connection established, session ID provided |
| `session.updated` | Session configuration acknowledged |
| `input_audio_buffer.speech_started` | VAD detected user speech (includes `audio_start_ms`) |
| `input_audio_buffer.speech_stopped` | VAD detected end of speech (includes `audio_end_ms`) |
| `input_audio_buffer.committed` | Audio buffer committed (by client or server VAD) |
| `input_audio_buffer.cleared` | Audio buffer cleared (confirmation) |
| `conversation.item.created` | New conversation item created (user message or AI response) |
| `conversation.item.input_audio_transcription.completed` | User speech transcribed |
| `response.created` | AI response generation started |
| `response.audio.delta` | Streaming audio response chunk |
| `response.audio_transcript.delta` | Streaming text of audio response |
| `response.function_call_arguments.delta` | Streaming function call args |
| `response.done` | Response complete |
| `error` | Error occurred |

### Client â†’ Server Commands

| Command | Description |
|---------|-------------|
| `session.update` | Configure session (first thing after connect) |
| `input_audio_buffer.append` | Send audio chunk (base64 encoded) |
| `input_audio_buffer.commit` | Manually commit audio (for `turn_detection: none`) |
| `input_audio_buffer.clear` | Clear audio buffer (reset for next turn) |
| `response.create` | Manually trigger response generation |
| `response.cancel` | Cancel in-progress response |
| `conversation.item.create` | Add item to conversation (text, tool response) |

---

## Audio Format Requirements

| Property | Value |
|----------|-------|
| Format | PCM 16-bit |
| Sample Rate | 24,000 Hz |
| Channels | Mono (1) |
| Encoding | Little-endian |
| Base64 | Yes (for WebSocket JSON) |

**Flutter mic_stream config:**
```dart
MicStream.microphone(
  sampleRate: 24000,
  channelConfig: ChannelConfig.CHANNEL_IN_MONO,
  audioFormat: AudioFormat.ENCODING_PCM_16BIT,
)
```

---

## Turn Detection Modes

### `server_vad` (Voice Activity Detection)
- Server automatically detects speech start/stop
- Triggers response generation on silence
- Best for natural conversation

```json
"turn_detection": {
  "type": "server_vad",
  "threshold": 0.5,
  "silence_duration_ms": 500
}
```

### `none` (Manual Control)
- Client controls turn boundaries
- Must call `input_audio_buffer.commit` + `response.create`
- Best for push-to-talk or file playback

```json
"turn_detection": null
```

---

## Implementation in Odelle

### Current Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         VoiceScreen                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ GestureDetector                                          â”‚   â”‚
â”‚  â”‚   onTap â†’ _connect()         (tap to connect)            â”‚   â”‚
â”‚  â”‚   onLongPressStart â†’ _startRecording()                   â”‚   â”‚
â”‚  â”‚   onLongPressEnd â†’ _stopRecording()                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                                  â”‚
â”‚                              â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ AzureSpeechService                                       â”‚   â”‚
â”‚  â”‚   - WebSocket connection                                 â”‚   â”‚
â”‚  â”‚   - Audio buffer management                              â”‚   â”‚
â”‚  â”‚   - Event callbacks                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                                  â”‚
â”‚                              â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ MicStream                                                â”‚   â”‚
â”‚  â”‚   - PCM16 @ 24kHz                                        â”‚   â”‚
â”‚  â”‚   - Mono channel                                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Callback Flow

```dart
// User transcription received
_speechService.onTranscription = (text) {
  // Save to journal, display
};

// Partial transcription (streaming)
_speechService.onPartialResult = (text) {
  // Show real-time text
};

// AI audio response
_speechService.onAudioResponse = (audioData) {
  // Play through speaker
};

// AI text response
_speechService.onTextResponse = (text) {
  // Display AI response text
};

// Speech detection
_speechService.onSpeechStarted = () {
  // Show "listening" indicator
};

_speechService.onSpeechStopped = () {
  // Processing...
};
```

---

## Environment Variables

```env
# Azure OpenAI Realtime API
AZURE_GPT_REALTIME_KEY=<your-api-key>
AZURE_GPT_REALTIME_DEPLOYMENT_URL=https://<resource>.cognitiveservices.azure.com/openai/realtime?api-version=2024-10-01-preview&deployment=<deployment>
```

---

## Current Implementation Status

### âœ… Completed
- **WebSocket Connection** - Connects to Azure OpenAI Realtime API
- **Session Configuration** - Configures session for conversation or transcription mode
- **Audio Streaming** - Streams PCM16 audio to Azure (24kHz mono)
- **Server VAD** - Voice Activity Detection for natural turn-taking
- **Audio Playback** - Plays AI audio responses through device speaker
- **Interruption Handling** - Stops AI audio when user starts speaking
- **Multi-Turn State Management** - Maintains recording state between turns
- **Diagnostic Logging** - Tracks audio chunks, VAD events, state transitions

### ğŸ”§ Recently Fixed (Jan 2026)
- Added `input_audio_buffer.cleared` event handler
- Added `input_audio_buffer.committed` event handler  
- Added `conversation.item.created` event handler
- State drift detection after `response.done`
- Auto-restore recording state for next turn
- Audio chunk tracking for debugging

### âš ï¸ Known Issues
- Multi-turn conversation may require testing after VAD fixes
- Buffer clear timing might need adjustment based on testing

---

## Multi-Turn Conversation Flow

```
TURN 1:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Client  â”‚                           â”‚  Azure   â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚                                      â”‚
       â”‚â”€â”€â”€ input_audio_buffer.append â”€â”€â”€â”€â”€â”€â”€â†’â”‚
       â”‚â”€â”€â”€ input_audio_buffer.append â”€â”€â”€â”€â”€â”€â”€â†’â”‚
       â”‚                                      â”‚
       â”‚â†â”€â”€ input_audio_buffer.speech_started â”‚
       â”‚                                      â”‚
       â”‚â†â”€â”€ input_audio_buffer.speech_stopped â”‚
       â”‚â†â”€â”€ input_audio_buffer.committed â”€â”€â”€â”€â”€â”‚
       â”‚â†â”€â”€ conversation.item.created â”€â”€â”€â”€â”€â”€â”€â”€â”‚
       â”‚                                      â”‚
       â”‚â†â”€â”€ response.created â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
       â”‚â†â”€â”€ response.audio.delta â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
       â”‚â†â”€â”€ response.audio_transcript.delta â”€â”€â”‚
       â”‚â†â”€â”€ response.done â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
       â”‚                                      â”‚
       â”‚â”€â”€â”€ input_audio_buffer.clear â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚ (reset for next turn)
       â”‚â†â”€â”€ input_audio_buffer.cleared â”€â”€â”€â”€â”€â”€â”€â”‚
       â”‚                                      â”‚

TURN 2+:
       â”‚â”€â”€â”€ input_audio_buffer.append â”€â”€â”€â”€â”€â”€â”€â†’â”‚ (new audio)
       â”‚â†â”€â”€ input_audio_buffer.speech_started â”‚ (VAD detects new speech)
       â”‚    ... (same flow as Turn 1) ...     â”‚
```

---

## Future Enhancements

### Phase 1: Conversation Polish
1. ~~**Multi-Turn Conversation**~~ âœ… - Fixed VAD state management
2. **Waveform Standardization** - Unify `VoiceWaveformAnimated` and `WaveformVisualizer` components
3. **Typing Animation** - Use `ConversationText` widget for AI response display

### Phase 2: Agentic Voice
4. **Tool Registration** - Dynamic tool registration for different screens
5. **Function Calling** - Enable AI to call functions during conversation
6. **Tool Response Handling** - Send tool results back to continue response

### Phase 3: Advanced Features
7. **Mode Switching UI** - Allow user to toggle between Live, Transcription-Only, and Agent modes
8. **Semantic VAD** - Use Azure's `semantic_vad` for smarter turn detection
9. **Offline Queue** - Queue recordings when offline, sync when connected
10. **Conversation History** - Persist and display conversation threads
